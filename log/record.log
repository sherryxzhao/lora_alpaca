12/11/2023 05:14:10 - INFO - __main__ -   name: Xuhan Zhao
12/11/2023 05:14:10 - INFO - __main__ -   GTID: 903443253
12/11/2023 05:14:10 - INFO - __main__ -   r: 8
12/11/2023 05:14:10 - INFO - __main__ -   lora_alpha: 32
12/11/2023 05:14:10 - INFO - __main__ -   lora_dropout: 0.1
12/11/2023 05:14:10 - INFO - __main__ -   output_dir: ./output
12/11/2023 05:14:10 - INFO - __main__ -   overwrite_output_dir: False
12/11/2023 05:14:10 - INFO - __main__ -   do_train: False
12/11/2023 05:14:10 - INFO - __main__ -   do_eval: False
12/11/2023 05:14:10 - INFO - __main__ -   do_predict: False
12/11/2023 05:14:10 - INFO - __main__ -   evaluation_strategy: no
12/11/2023 05:14:10 - INFO - __main__ -   prediction_loss_only: False
12/11/2023 05:14:10 - INFO - __main__ -   per_device_train_batch_size: 2
12/11/2023 05:14:10 - INFO - __main__ -   per_device_eval_batch_size: 2
12/11/2023 05:14:10 - INFO - __main__ -   per_gpu_train_batch_size: None
12/11/2023 05:14:10 - INFO - __main__ -   per_gpu_eval_batch_size: None
12/11/2023 05:14:10 - INFO - __main__ -   gradient_accumulation_steps: 16
12/11/2023 05:14:10 - INFO - __main__ -   eval_accumulation_steps: None
12/11/2023 05:14:10 - INFO - __main__ -   eval_delay: 0
12/11/2023 05:14:10 - INFO - __main__ -   learning_rate: 0.0001
12/11/2023 05:14:10 - INFO - __main__ -   weight_decay: 0.0
12/11/2023 05:14:10 - INFO - __main__ -   adam_beta1: 0.9
12/11/2023 05:14:10 - INFO - __main__ -   adam_beta2: 0.999
12/11/2023 05:14:10 - INFO - __main__ -   adam_epsilon: 1e-08
12/11/2023 05:14:10 - INFO - __main__ -   max_grad_norm: 1.0
12/11/2023 05:14:10 - INFO - __main__ -   num_train_epochs: 3.0
12/11/2023 05:14:10 - INFO - __main__ -   max_steps: -1
12/11/2023 05:14:10 - INFO - __main__ -   lr_scheduler_type: cosine
12/11/2023 05:14:10 - INFO - __main__ -   warmup_ratio: 0.03
12/11/2023 05:14:10 - INFO - __main__ -   warmup_steps: 0
12/11/2023 05:14:10 - INFO - __main__ -   log_level: passive
12/11/2023 05:14:10 - INFO - __main__ -   log_level_replica: warning
12/11/2023 05:14:10 - INFO - __main__ -   log_on_each_node: True
12/11/2023 05:14:10 - INFO - __main__ -   logging_dir: ./traininglog
12/11/2023 05:14:10 - INFO - __main__ -   logging_strategy: steps
12/11/2023 05:14:10 - INFO - __main__ -   logging_first_step: False
12/11/2023 05:14:10 - INFO - __main__ -   logging_steps: 10
12/11/2023 05:14:10 - INFO - __main__ -   logging_nan_inf_filter: True
12/11/2023 05:14:10 - INFO - __main__ -   save_strategy: steps
12/11/2023 05:14:10 - INFO - __main__ -   save_steps: 2000
12/11/2023 05:14:10 - INFO - __main__ -   save_total_limit: None
12/11/2023 05:14:10 - INFO - __main__ -   save_safetensors: False
12/11/2023 05:14:10 - INFO - __main__ -   save_on_each_node: False
12/11/2023 05:14:10 - INFO - __main__ -   no_cuda: False
12/11/2023 05:14:10 - INFO - __main__ -   use_mps_device: False
12/11/2023 05:14:10 - INFO - __main__ -   seed: 42
12/11/2023 05:14:10 - INFO - __main__ -   data_seed: None
12/11/2023 05:14:10 - INFO - __main__ -   jit_mode_eval: False
12/11/2023 05:14:10 - INFO - __main__ -   use_ipex: False
12/11/2023 05:14:10 - INFO - __main__ -   bf16: False
12/11/2023 05:14:10 - INFO - __main__ -   fp16: True
12/11/2023 05:14:10 - INFO - __main__ -   fp16_opt_level: O1
12/11/2023 05:14:10 - INFO - __main__ -   half_precision_backend: auto
12/11/2023 05:14:10 - INFO - __main__ -   bf16_full_eval: False
12/11/2023 05:14:10 - INFO - __main__ -   fp16_full_eval: False
12/11/2023 05:14:10 - INFO - __main__ -   tf32: None
12/11/2023 05:14:10 - INFO - __main__ -   local_rank: 0
12/11/2023 05:14:10 - INFO - __main__ -   xpu_backend: None
12/11/2023 05:14:10 - INFO - __main__ -   tpu_num_cores: None
12/11/2023 05:14:10 - INFO - __main__ -   tpu_metrics_debug: False
12/11/2023 05:14:10 - INFO - __main__ -   debug: []
12/11/2023 05:14:10 - INFO - __main__ -   dataloader_drop_last: False
12/11/2023 05:14:10 - INFO - __main__ -   eval_steps: None
12/11/2023 05:14:10 - INFO - __main__ -   dataloader_num_workers: 0
12/11/2023 05:14:10 - INFO - __main__ -   past_index: -1
12/11/2023 05:14:10 - INFO - __main__ -   run_name: ./output
12/11/2023 05:14:10 - INFO - __main__ -   disable_tqdm: False
12/11/2023 05:14:10 - INFO - __main__ -   remove_unused_columns: True
12/11/2023 05:14:10 - INFO - __main__ -   label_names: None
12/11/2023 05:14:10 - INFO - __main__ -   load_best_model_at_end: False
12/11/2023 05:14:10 - INFO - __main__ -   metric_for_best_model: None
12/11/2023 05:14:10 - INFO - __main__ -   greater_is_better: None
12/11/2023 05:14:10 - INFO - __main__ -   ignore_data_skip: False
12/11/2023 05:14:10 - INFO - __main__ -   sharded_ddp: []
12/11/2023 05:14:10 - INFO - __main__ -   fsdp: []
12/11/2023 05:14:10 - INFO - __main__ -   fsdp_min_num_params: 0
12/11/2023 05:14:10 - INFO - __main__ -   fsdp_config: {'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}
12/11/2023 05:14:10 - INFO - __main__ -   fsdp_transformer_layer_cls_to_wrap: None
12/11/2023 05:14:10 - INFO - __main__ -   deepspeed: /home/zhouh/temp_zhaoxh/lora_alpaca/configs/stage2.json
12/11/2023 05:14:10 - INFO - __main__ -   label_smoothing_factor: 0.0
12/11/2023 05:14:10 - INFO - __main__ -   optim: adamw_torch
12/11/2023 05:14:10 - INFO - __main__ -   optim_args: None
12/11/2023 05:14:10 - INFO - __main__ -   adafactor: False
12/11/2023 05:14:10 - INFO - __main__ -   group_by_length: True
12/11/2023 05:14:10 - INFO - __main__ -   length_column_name: length
12/11/2023 05:14:10 - INFO - __main__ -   report_to: []
12/11/2023 05:14:10 - INFO - __main__ -   ddp_find_unused_parameters: None
12/11/2023 05:14:10 - INFO - __main__ -   ddp_bucket_cap_mb: None
12/11/2023 05:14:10 - INFO - __main__ -   dataloader_pin_memory: True
12/11/2023 05:14:10 - INFO - __main__ -   skip_memory_metrics: True
12/11/2023 05:14:10 - INFO - __main__ -   use_legacy_prediction_loop: False
12/11/2023 05:14:10 - INFO - __main__ -   push_to_hub: False
12/11/2023 05:14:10 - INFO - __main__ -   resume_from_checkpoint: None
12/11/2023 05:14:10 - INFO - __main__ -   hub_model_id: None
12/11/2023 05:14:10 - INFO - __main__ -   hub_strategy: every_save
12/11/2023 05:14:10 - INFO - __main__ -   hub_token: None
12/11/2023 05:14:10 - INFO - __main__ -   hub_private_repo: False
12/11/2023 05:14:10 - INFO - __main__ -   gradient_checkpointing: False
12/11/2023 05:14:10 - INFO - __main__ -   include_inputs_for_metrics: False
12/11/2023 05:14:10 - INFO - __main__ -   fp16_backend: auto
12/11/2023 05:14:10 - INFO - __main__ -   push_to_hub_model_id: None
12/11/2023 05:14:10 - INFO - __main__ -   push_to_hub_organization: None
12/11/2023 05:14:10 - INFO - __main__ -   push_to_hub_token: None
12/11/2023 05:14:10 - INFO - __main__ -   _n_gpu: 1
12/11/2023 05:14:10 - INFO - __main__ -   mp_parameters: 
12/11/2023 05:14:10 - INFO - __main__ -   auto_find_batch_size: False
12/11/2023 05:14:10 - INFO - __main__ -   full_determinism: False
12/11/2023 05:14:10 - INFO - __main__ -   torchdynamo: None
12/11/2023 05:14:10 - INFO - __main__ -   ray_scope: last
12/11/2023 05:14:10 - INFO - __main__ -   ddp_timeout: 1800
12/11/2023 05:14:10 - INFO - __main__ -   torch_compile: False
12/11/2023 05:14:10 - INFO - __main__ -   torch_compile_backend: None
12/11/2023 05:14:10 - INFO - __main__ -   torch_compile_mode: None
12/11/2023 05:14:10 - INFO - __main__ -   cache_dir: ./cache
12/11/2023 05:14:10 - INFO - __main__ -   model_max_length: 512
12/11/2023 05:16:34 - INFO - __main__ -   LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (1): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
12/11/2023 05:17:00 - WARNING - root -   Loading data...
12/11/2023 05:17:00 - WARNING - root -   Formatting inputs...
12/11/2023 05:17:00 - WARNING - root -   Tokenizing inputs... This may take some time...
12/11/2023 05:18:07 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0
12/11/2023 05:18:11 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
12/11/2023 05:18:52 - INFO - __main__ -   {'loss': 1.5273, 'learning_rate': 6.37673065048409e-05, 'epoch': 0.02}
12/11/2023 05:19:27 - INFO - __main__ -   {'loss': 1.46, 'learning_rate': 8.296317850549692e-05, 'epoch': 0.05}
12/11/2023 05:20:02 - INFO - __main__ -   {'loss': 1.198, 'learning_rate': 9.419204379452388e-05, 'epoch': 0.07}
12/11/2023 05:20:36 - INFO - __main__ -   {'loss': 1.1196, 'learning_rate': 0.0001, 'epoch': 0.1}
12/11/2023 05:21:08 - INFO - __main__ -   {'loss': 1.0567, 'learning_rate': 0.0001, 'epoch': 0.12}
12/11/2023 05:21:42 - INFO - __main__ -   {'loss': 1.1029, 'learning_rate': 0.0001, 'epoch': 0.15}
12/11/2023 05:22:18 - INFO - __main__ -   {'loss': 1.1255, 'learning_rate': 0.0001, 'epoch': 0.17}
12/11/2023 05:22:53 - INFO - __main__ -   {'loss': 1.0683, 'learning_rate': 0.0001, 'epoch': 0.2}
12/11/2023 05:23:27 - INFO - __main__ -   {'loss': 1.0351, 'learning_rate': 0.0001, 'epoch': 0.22}
12/11/2023 05:24:00 - INFO - __main__ -   {'loss': 1.0329, 'learning_rate': 0.0001, 'epoch': 0.25}
12/11/2023 05:24:35 - INFO - __main__ -   {'loss': 1.0641, 'learning_rate': 0.0001, 'epoch': 0.27}
12/11/2023 05:25:10 - INFO - __main__ -   {'loss': 1.0583, 'learning_rate': 0.0001, 'epoch': 0.3}
12/11/2023 05:25:45 - INFO - __main__ -   {'loss': 1.0543, 'learning_rate': 0.0001, 'epoch': 0.32}
12/11/2023 05:26:19 - INFO - __main__ -   {'loss': 0.9843, 'learning_rate': 0.0001, 'epoch': 0.34}
12/11/2023 05:26:50 - INFO - __main__ -   {'loss': 1.0097, 'learning_rate': 0.0001, 'epoch': 0.37}
12/11/2023 05:27:25 - INFO - __main__ -   {'loss': 1.0622, 'learning_rate': 0.0001, 'epoch': 0.39}
12/11/2023 05:28:00 - INFO - __main__ -   {'loss': 1.0649, 'learning_rate': 0.0001, 'epoch': 0.42}
12/11/2023 05:28:36 - INFO - __main__ -   {'loss': 1.0296, 'learning_rate': 0.0001, 'epoch': 0.44}
12/11/2023 05:29:10 - INFO - __main__ -   {'loss': 1.0057, 'learning_rate': 0.0001, 'epoch': 0.47}
12/11/2023 05:29:41 - INFO - __main__ -   {'loss': 0.9902, 'learning_rate': 0.0001, 'epoch': 0.49}
12/11/2023 05:30:16 - INFO - __main__ -   {'loss': 1.0666, 'learning_rate': 0.0001, 'epoch': 0.52}
12/11/2023 05:30:52 - INFO - __main__ -   {'loss': 1.0897, 'learning_rate': 0.0001, 'epoch': 0.54}
12/11/2023 05:31:27 - INFO - __main__ -   {'loss': 1.0096, 'learning_rate': 0.0001, 'epoch': 0.57}
12/11/2023 05:32:01 - INFO - __main__ -   {'loss': 0.9961, 'learning_rate': 0.0001, 'epoch': 0.59}
12/11/2023 05:32:32 - INFO - __main__ -   {'loss': 0.9683, 'learning_rate': 0.0001, 'epoch': 0.62}
12/11/2023 05:33:06 - INFO - __main__ -   {'loss': 1.0347, 'learning_rate': 0.0001, 'epoch': 0.64}
12/11/2023 05:33:42 - INFO - __main__ -   {'loss': 1.0522, 'learning_rate': 0.0001, 'epoch': 0.66}
12/11/2023 05:34:16 - INFO - __main__ -   {'loss': 0.993, 'learning_rate': 0.0001, 'epoch': 0.69}
12/11/2023 05:34:50 - INFO - __main__ -   {'loss': 0.9794, 'learning_rate': 0.0001, 'epoch': 0.71}
12/11/2023 05:35:22 - INFO - __main__ -   {'loss': 0.9741, 'learning_rate': 0.0001, 'epoch': 0.74}
12/11/2023 05:35:57 - INFO - __main__ -   {'loss': 1.0424, 'learning_rate': 0.0001, 'epoch': 0.76}
12/11/2023 05:36:33 - INFO - __main__ -   {'loss': 1.0561, 'learning_rate': 0.0001, 'epoch': 0.79}
12/11/2023 05:37:07 - INFO - __main__ -   {'loss': 0.9991, 'learning_rate': 0.0001, 'epoch': 0.81}
12/11/2023 05:37:42 - INFO - __main__ -   {'loss': 0.9678, 'learning_rate': 0.0001, 'epoch': 0.84}
12/11/2023 05:38:14 - INFO - __main__ -   {'loss': 0.954, 'learning_rate': 0.0001, 'epoch': 0.86}
12/11/2023 05:38:48 - INFO - __main__ -   {'loss': 1.0219, 'learning_rate': 0.0001, 'epoch': 0.89}
12/11/2023 05:39:24 - INFO - __main__ -   {'loss': 1.0425, 'learning_rate': 0.0001, 'epoch': 0.91}
12/11/2023 05:39:58 - INFO - __main__ -   {'loss': 1.0022, 'learning_rate': 0.0001, 'epoch': 0.94}
12/11/2023 05:40:32 - INFO - __main__ -   {'loss': 0.9626, 'learning_rate': 0.0001, 'epoch': 0.96}
12/11/2023 05:41:03 - INFO - __main__ -   {'loss': 0.9674, 'learning_rate': 0.0001, 'epoch': 0.98}
12/11/2023 05:41:41 - INFO - __main__ -   {'loss': 1.0795, 'learning_rate': 0.0001, 'epoch': 1.01}
12/11/2023 05:42:14 - INFO - __main__ -   {'loss': 0.9148, 'learning_rate': 0.0001, 'epoch': 1.03}
12/11/2023 05:42:47 - INFO - __main__ -   {'loss': 0.9781, 'learning_rate': 0.0001, 'epoch': 1.06}
12/11/2023 05:43:22 - INFO - __main__ -   {'loss': 1.0303, 'learning_rate': 0.0001, 'epoch': 1.08}
12/11/2023 05:43:57 - INFO - __main__ -   {'loss': 0.9988, 'learning_rate': 0.0001, 'epoch': 1.11}
12/11/2023 05:44:32 - INFO - __main__ -   {'loss': 0.979, 'learning_rate': 0.0001, 'epoch': 1.13}
12/11/2023 05:45:06 - INFO - __main__ -   {'loss': 0.9505, 'learning_rate': 0.0001, 'epoch': 1.16}
12/11/2023 05:45:39 - INFO - __main__ -   {'loss': 0.985, 'learning_rate': 0.0001, 'epoch': 1.18}
12/11/2023 05:46:14 - INFO - __main__ -   {'loss': 1.0583, 'learning_rate': 0.0001, 'epoch': 1.21}
12/11/2023 05:46:49 - INFO - __main__ -   {'loss': 1.0062, 'learning_rate': 0.0001, 'epoch': 1.23}
12/11/2023 05:47:24 - INFO - __main__ -   {'loss': 0.9488, 'learning_rate': 0.0001, 'epoch': 1.26}
12/11/2023 05:47:58 - INFO - __main__ -   {'loss': 0.924, 'learning_rate': 0.0001, 'epoch': 1.28}
12/11/2023 05:48:30 - INFO - __main__ -   {'loss': 0.9577, 'learning_rate': 0.0001, 'epoch': 1.3}
12/11/2023 05:49:06 - INFO - __main__ -   {'loss': 1.0335, 'learning_rate': 0.0001, 'epoch': 1.33}
12/11/2023 05:49:41 - INFO - __main__ -   {'loss': 0.985, 'learning_rate': 0.0001, 'epoch': 1.35}
12/11/2023 05:50:16 - INFO - __main__ -   {'loss': 0.9344, 'learning_rate': 0.0001, 'epoch': 1.38}
12/11/2023 05:50:49 - INFO - __main__ -   {'loss': 0.9518, 'learning_rate': 0.0001, 'epoch': 1.4}
12/11/2023 05:51:21 - INFO - __main__ -   {'loss': 0.9729, 'learning_rate': 0.0001, 'epoch': 1.43}
12/11/2023 05:51:56 - INFO - __main__ -   {'loss': 1.0291, 'learning_rate': 0.0001, 'epoch': 1.45}
12/11/2023 05:52:31 - INFO - __main__ -   {'loss': 1.0156, 'learning_rate': 0.0001, 'epoch': 1.48}
12/11/2023 05:53:05 - INFO - __main__ -   {'loss': 0.9502, 'learning_rate': 0.0001, 'epoch': 1.5}
12/11/2023 05:53:38 - INFO - __main__ -   {'loss': 0.953, 'learning_rate': 0.0001, 'epoch': 1.53}
12/11/2023 05:54:12 - INFO - __main__ -   {'loss': 0.9673, 'learning_rate': 0.0001, 'epoch': 1.55}
12/11/2023 05:54:48 - INFO - __main__ -   {'loss': 1.041, 'learning_rate': 0.0001, 'epoch': 1.58}
12/11/2023 05:55:23 - INFO - __main__ -   {'loss': 0.9828, 'learning_rate': 0.0001, 'epoch': 1.6}
12/11/2023 05:55:57 - INFO - __main__ -   {'loss': 0.9769, 'learning_rate': 0.0001, 'epoch': 1.62}
12/11/2023 05:56:31 - INFO - __main__ -   {'loss': 0.9268, 'learning_rate': 0.0001, 'epoch': 1.65}
12/11/2023 05:57:03 - INFO - __main__ -   {'loss': 0.9751, 'learning_rate': 0.0001, 'epoch': 1.67}
12/11/2023 05:57:38 - INFO - __main__ -   {'loss': 1.0229, 'learning_rate': 0.0001, 'epoch': 1.7}
12/11/2023 05:58:13 - INFO - __main__ -   {'loss': 0.9822, 'learning_rate': 0.0001, 'epoch': 1.72}
12/11/2023 05:58:48 - INFO - __main__ -   {'loss': 0.972, 'learning_rate': 0.0001, 'epoch': 1.75}
12/11/2023 05:59:22 - INFO - __main__ -   {'loss': 0.9351, 'learning_rate': 0.0001, 'epoch': 1.77}
12/11/2023 05:59:54 - INFO - __main__ -   {'loss': 0.9872, 'learning_rate': 0.0001, 'epoch': 1.8}
12/11/2023 06:00:30 - INFO - __main__ -   {'loss': 1.0309, 'learning_rate': 0.0001, 'epoch': 1.82}
12/11/2023 06:01:04 - INFO - __main__ -   {'loss': 0.999, 'learning_rate': 0.0001, 'epoch': 1.85}
12/11/2023 06:01:39 - INFO - __main__ -   {'loss': 0.9393, 'learning_rate': 0.0001, 'epoch': 1.87}
12/11/2023 06:02:12 - INFO - __main__ -   {'loss': 0.9166, 'learning_rate': 0.0001, 'epoch': 1.9}
12/11/2023 06:02:44 - INFO - __main__ -   {'loss': 0.9913, 'learning_rate': 0.0001, 'epoch': 1.92}
12/11/2023 06:03:19 - INFO - __main__ -   {'loss': 1.0428, 'learning_rate': 0.0001, 'epoch': 1.94}
12/11/2023 06:03:55 - INFO - __main__ -   {'loss': 1.0047, 'learning_rate': 0.0001, 'epoch': 1.97}
12/11/2023 06:04:27 - INFO - __main__ -   {'loss': 0.9328, 'learning_rate': 0.0001, 'epoch': 1.99}
12/11/2023 06:05:02 - INFO - __main__ -   {'loss': 1.014, 'learning_rate': 0.0001, 'epoch': 2.02}
12/11/2023 06:05:38 - INFO - __main__ -   {'loss': 0.9358, 'learning_rate': 0.0001, 'epoch': 2.04}
12/11/2023 06:06:12 - INFO - __main__ -   {'loss': 0.89, 'learning_rate': 0.0001, 'epoch': 2.07}
12/11/2023 06:06:44 - INFO - __main__ -   {'loss': 0.938, 'learning_rate': 0.0001, 'epoch': 2.09}
12/11/2023 06:07:19 - INFO - __main__ -   {'loss': 0.9962, 'learning_rate': 0.0001, 'epoch': 2.12}
12/11/2023 06:07:53 - INFO - __main__ -   {'loss': 1.0187, 'learning_rate': 0.0001, 'epoch': 2.14}
12/11/2023 06:08:28 - INFO - __main__ -   {'loss': 0.9828, 'learning_rate': 0.0001, 'epoch': 2.17}
12/11/2023 06:09:02 - INFO - __main__ -   {'loss': 0.9067, 'learning_rate': 0.0001, 'epoch': 2.19}
12/11/2023 06:09:34 - INFO - __main__ -   {'loss': 0.8886, 'learning_rate': 0.0001, 'epoch': 2.22}
12/11/2023 06:10:09 - INFO - __main__ -   {'loss': 0.9888, 'learning_rate': 0.0001, 'epoch': 2.24}
12/11/2023 06:10:44 - INFO - __main__ -   {'loss': 0.9714, 'learning_rate': 0.0001, 'epoch': 2.26}
12/11/2023 06:11:19 - INFO - __main__ -   {'loss': 0.9413, 'learning_rate': 0.0001, 'epoch': 2.29}
12/11/2023 06:11:53 - INFO - __main__ -   {'loss': 0.8937, 'learning_rate': 0.0001, 'epoch': 2.31}
12/11/2023 06:12:24 - INFO - __main__ -   {'loss': 0.9197, 'learning_rate': 0.0001, 'epoch': 2.34}
12/11/2023 06:12:59 - INFO - __main__ -   {'loss': 1.0138, 'learning_rate': 0.0001, 'epoch': 2.36}
12/11/2023 06:13:34 - INFO - __main__ -   {'loss': 1.0128, 'learning_rate': 0.0001, 'epoch': 2.39}
12/11/2023 06:14:10 - INFO - __main__ -   {'loss': 0.9349, 'learning_rate': 0.0001, 'epoch': 2.41}
12/11/2023 06:14:44 - INFO - __main__ -   {'loss': 0.9231, 'learning_rate': 0.0001, 'epoch': 2.44}
12/11/2023 06:15:15 - INFO - __main__ -   {'loss': 0.9019, 'learning_rate': 0.0001, 'epoch': 2.46}
12/11/2023 06:15:51 - INFO - __main__ -   {'loss': 1.012, 'learning_rate': 0.0001, 'epoch': 2.49}
12/11/2023 06:16:26 - INFO - __main__ -   {'loss': 0.9888, 'learning_rate': 0.0001, 'epoch': 2.51}
12/11/2023 06:17:01 - INFO - __main__ -   {'loss': 0.9323, 'learning_rate': 0.0001, 'epoch': 2.53}
12/11/2023 06:17:35 - INFO - __main__ -   {'loss': 0.8974, 'learning_rate': 0.0001, 'epoch': 2.56}
12/11/2023 06:18:06 - INFO - __main__ -   {'loss': 0.9052, 'learning_rate': 0.0001, 'epoch': 2.58}
12/11/2023 06:18:40 - INFO - __main__ -   {'loss': 1.002, 'learning_rate': 0.0001, 'epoch': 2.61}
12/11/2023 06:19:15 - INFO - __main__ -   {'loss': 1.0273, 'learning_rate': 0.0001, 'epoch': 2.63}
12/11/2023 06:19:50 - INFO - __main__ -   {'loss': 0.9483, 'learning_rate': 0.0001, 'epoch': 2.66}
12/11/2023 06:20:24 - INFO - __main__ -   {'loss': 0.9152, 'learning_rate': 0.0001, 'epoch': 2.68}
12/11/2023 06:20:55 - INFO - __main__ -   {'loss': 0.9101, 'learning_rate': 0.0001, 'epoch': 2.71}
12/11/2023 06:21:30 - INFO - __main__ -   {'loss': 1.0214, 'learning_rate': 0.0001, 'epoch': 2.73}
12/11/2023 06:22:05 - INFO - __main__ -   {'loss': 1.0161, 'learning_rate': 0.0001, 'epoch': 2.76}
12/11/2023 06:22:41 - INFO - __main__ -   {'loss': 0.9458, 'learning_rate': 0.0001, 'epoch': 2.78}
12/11/2023 06:23:15 - INFO - __main__ -   {'loss': 0.8979, 'learning_rate': 0.0001, 'epoch': 2.81}
12/11/2023 06:23:46 - INFO - __main__ -   {'loss': 0.9234, 'learning_rate': 0.0001, 'epoch': 2.83}
12/11/2023 06:24:22 - INFO - __main__ -   {'loss': 0.9949, 'learning_rate': 0.0001, 'epoch': 2.85}
12/11/2023 06:24:57 - INFO - __main__ -   {'loss': 0.9992, 'learning_rate': 0.0001, 'epoch': 2.88}
12/11/2023 06:25:32 - INFO - __main__ -   {'loss': 0.9469, 'learning_rate': 0.0001, 'epoch': 2.9}
12/11/2023 06:26:06 - INFO - __main__ -   {'loss': 0.9303, 'learning_rate': 0.0001, 'epoch': 2.93}
12/11/2023 06:26:37 - INFO - __main__ -   {'loss': 0.9434, 'learning_rate': 0.0001, 'epoch': 2.95}
12/11/2023 06:27:12 - INFO - __main__ -   {'loss': 1.0231, 'learning_rate': 0.0001, 'epoch': 2.98}
12/11/2023 06:27:38 - INFO - __main__ -   {'train_runtime': 4161.5876, 'train_samples_per_second': 37.487, 'train_steps_per_second': 0.293, 'total_flos': 7.890014711641539e+17, 'train_loss': 0.9970626697947435, 'epoch': 3.0}
