12/08/2023 15:29:15 - INFO - __main__ -   name: Xuhan Zhao
12/08/2023 15:29:15 - INFO - __main__ -   GTID: 903443253
12/08/2023 15:29:15 - INFO - __main__ -   r: 8
12/08/2023 15:29:15 - INFO - __main__ -   lora_alpha: 32
12/08/2023 15:29:15 - INFO - __main__ -   lora_dropout: 0.1
12/08/2023 15:29:15 - INFO - __main__ -   output_dir: ./output
12/08/2023 15:29:15 - INFO - __main__ -   overwrite_output_dir: False
12/08/2023 15:29:15 - INFO - __main__ -   do_train: False
12/08/2023 15:29:15 - INFO - __main__ -   do_eval: False
12/08/2023 15:29:15 - INFO - __main__ -   do_predict: False
12/08/2023 15:29:15 - INFO - __main__ -   evaluation_strategy: no
12/08/2023 15:29:15 - INFO - __main__ -   prediction_loss_only: False
12/08/2023 15:29:15 - INFO - __main__ -   per_device_train_batch_size: 2
12/08/2023 15:29:15 - INFO - __main__ -   per_device_eval_batch_size: 2
12/08/2023 15:29:15 - INFO - __main__ -   per_gpu_train_batch_size: None
12/08/2023 15:29:15 - INFO - __main__ -   per_gpu_eval_batch_size: None
12/08/2023 15:29:15 - INFO - __main__ -   gradient_accumulation_steps: 16
12/08/2023 15:29:15 - INFO - __main__ -   eval_accumulation_steps: None
12/08/2023 15:29:15 - INFO - __main__ -   eval_delay: 0
12/08/2023 15:29:15 - INFO - __main__ -   learning_rate: 0.0001
12/08/2023 15:29:15 - INFO - __main__ -   weight_decay: 0.0
12/08/2023 15:29:15 - INFO - __main__ -   adam_beta1: 0.9
12/08/2023 15:29:15 - INFO - __main__ -   adam_beta2: 0.999
12/08/2023 15:29:15 - INFO - __main__ -   adam_epsilon: 1e-08
12/08/2023 15:29:15 - INFO - __main__ -   max_grad_norm: 1.0
12/08/2023 15:29:15 - INFO - __main__ -   num_train_epochs: 3.0
12/08/2023 15:29:15 - INFO - __main__ -   max_steps: -1
12/08/2023 15:29:15 - INFO - __main__ -   lr_scheduler_type: cosine
12/08/2023 15:29:15 - INFO - __main__ -   warmup_ratio: 0.03
12/08/2023 15:29:15 - INFO - __main__ -   warmup_steps: 0
12/08/2023 15:29:15 - INFO - __main__ -   log_level: passive
12/08/2023 15:29:15 - INFO - __main__ -   log_level_replica: warning
12/08/2023 15:29:15 - INFO - __main__ -   log_on_each_node: True
12/08/2023 15:29:15 - INFO - __main__ -   logging_dir: ./traininglog
12/08/2023 15:29:15 - INFO - __main__ -   logging_strategy: steps
12/08/2023 15:29:15 - INFO - __main__ -   logging_first_step: False
12/08/2023 15:29:15 - INFO - __main__ -   logging_steps: 10
12/08/2023 15:29:15 - INFO - __main__ -   logging_nan_inf_filter: True
12/08/2023 15:29:15 - INFO - __main__ -   save_strategy: steps
12/08/2023 15:29:15 - INFO - __main__ -   save_steps: 2000
12/08/2023 15:29:15 - INFO - __main__ -   save_total_limit: None
12/08/2023 15:29:15 - INFO - __main__ -   save_safetensors: False
12/08/2023 15:29:15 - INFO - __main__ -   save_on_each_node: False
12/08/2023 15:29:15 - INFO - __main__ -   no_cuda: False
12/08/2023 15:29:15 - INFO - __main__ -   use_mps_device: False
12/08/2023 15:29:15 - INFO - __main__ -   seed: 42
12/08/2023 15:29:15 - INFO - __main__ -   data_seed: None
12/08/2023 15:29:15 - INFO - __main__ -   jit_mode_eval: False
12/08/2023 15:29:15 - INFO - __main__ -   use_ipex: False
12/08/2023 15:29:15 - INFO - __main__ -   bf16: False
12/08/2023 15:29:15 - INFO - __main__ -   fp16: True
12/08/2023 15:29:15 - INFO - __main__ -   fp16_opt_level: O1
12/08/2023 15:29:15 - INFO - __main__ -   half_precision_backend: auto
12/08/2023 15:29:15 - INFO - __main__ -   bf16_full_eval: False
12/08/2023 15:29:15 - INFO - __main__ -   fp16_full_eval: False
12/08/2023 15:29:15 - INFO - __main__ -   tf32: None
12/08/2023 15:29:15 - INFO - __main__ -   local_rank: 0
12/08/2023 15:29:15 - INFO - __main__ -   xpu_backend: None
12/08/2023 15:29:15 - INFO - __main__ -   tpu_num_cores: None
12/08/2023 15:29:15 - INFO - __main__ -   tpu_metrics_debug: False
12/08/2023 15:29:15 - INFO - __main__ -   debug: []
12/08/2023 15:29:15 - INFO - __main__ -   dataloader_drop_last: False
12/08/2023 15:29:15 - INFO - __main__ -   eval_steps: None
12/08/2023 15:29:15 - INFO - __main__ -   dataloader_num_workers: 0
12/08/2023 15:29:15 - INFO - __main__ -   past_index: -1
12/08/2023 15:29:15 - INFO - __main__ -   run_name: ./output
12/08/2023 15:29:15 - INFO - __main__ -   disable_tqdm: False
12/08/2023 15:29:15 - INFO - __main__ -   remove_unused_columns: True
12/08/2023 15:29:15 - INFO - __main__ -   label_names: None
12/08/2023 15:29:15 - INFO - __main__ -   load_best_model_at_end: False
12/08/2023 15:29:15 - INFO - __main__ -   metric_for_best_model: None
12/08/2023 15:29:15 - INFO - __main__ -   greater_is_better: None
12/08/2023 15:29:15 - INFO - __main__ -   ignore_data_skip: False
12/08/2023 15:29:15 - INFO - __main__ -   sharded_ddp: []
12/08/2023 15:29:15 - INFO - __main__ -   fsdp: []
12/08/2023 15:29:15 - INFO - __main__ -   fsdp_min_num_params: 0
12/08/2023 15:29:15 - INFO - __main__ -   fsdp_config: {'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}
12/08/2023 15:29:15 - INFO - __main__ -   fsdp_transformer_layer_cls_to_wrap: None
12/08/2023 15:29:15 - INFO - __main__ -   deepspeed: /home/zhouh/temp_zhaoxh/lora_alpaca/configs/stage2.json
12/08/2023 15:29:15 - INFO - __main__ -   label_smoothing_factor: 0.0
12/08/2023 15:29:15 - INFO - __main__ -   optim: adamw_torch
12/08/2023 15:29:15 - INFO - __main__ -   optim_args: None
12/08/2023 15:29:15 - INFO - __main__ -   adafactor: False
12/08/2023 15:29:15 - INFO - __main__ -   group_by_length: True
12/08/2023 15:29:15 - INFO - __main__ -   length_column_name: length
12/08/2023 15:29:15 - INFO - __main__ -   report_to: []
12/08/2023 15:29:15 - INFO - __main__ -   ddp_find_unused_parameters: None
12/08/2023 15:29:15 - INFO - __main__ -   ddp_bucket_cap_mb: None
12/08/2023 15:29:15 - INFO - __main__ -   dataloader_pin_memory: True
12/08/2023 15:29:15 - INFO - __main__ -   skip_memory_metrics: True
12/08/2023 15:29:15 - INFO - __main__ -   use_legacy_prediction_loop: False
12/08/2023 15:29:15 - INFO - __main__ -   push_to_hub: False
12/08/2023 15:29:15 - INFO - __main__ -   resume_from_checkpoint: None
12/08/2023 15:29:15 - INFO - __main__ -   hub_model_id: None
12/08/2023 15:29:15 - INFO - __main__ -   hub_strategy: every_save
12/08/2023 15:29:15 - INFO - __main__ -   hub_token: None
12/08/2023 15:29:15 - INFO - __main__ -   hub_private_repo: False
12/08/2023 15:29:15 - INFO - __main__ -   gradient_checkpointing: False
12/08/2023 15:29:15 - INFO - __main__ -   include_inputs_for_metrics: False
12/08/2023 15:29:15 - INFO - __main__ -   fp16_backend: auto
12/08/2023 15:29:15 - INFO - __main__ -   push_to_hub_model_id: None
12/08/2023 15:29:15 - INFO - __main__ -   push_to_hub_organization: None
12/08/2023 15:29:15 - INFO - __main__ -   push_to_hub_token: None
12/08/2023 15:29:15 - INFO - __main__ -   _n_gpu: 1
12/08/2023 15:29:15 - INFO - __main__ -   mp_parameters: 
12/08/2023 15:29:15 - INFO - __main__ -   auto_find_batch_size: False
12/08/2023 15:29:15 - INFO - __main__ -   full_determinism: False
12/08/2023 15:29:15 - INFO - __main__ -   torchdynamo: None
12/08/2023 15:29:15 - INFO - __main__ -   ray_scope: last
12/08/2023 15:29:15 - INFO - __main__ -   ddp_timeout: 1800
12/08/2023 15:29:15 - INFO - __main__ -   torch_compile: False
12/08/2023 15:29:15 - INFO - __main__ -   torch_compile_backend: None
12/08/2023 15:29:15 - INFO - __main__ -   torch_compile_mode: None
12/08/2023 15:29:15 - INFO - __main__ -   cache_dir: ./cache
12/08/2023 15:29:15 - INFO - __main__ -   model_max_length: 512
12/08/2023 15:32:05 - WARNING - root -   Loading data...
12/08/2023 15:32:05 - WARNING - root -   Formatting inputs...
12/08/2023 15:32:05 - WARNING - root -   Tokenizing inputs... This may take some time...
12/08/2023 15:33:12 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0
12/08/2023 15:33:14 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
12/08/2023 15:33:56 - INFO - __main__ -   {'loss': 1.5303, 'learning_rate': 6.37673065048409e-05, 'epoch': 0.02}
12/08/2023 15:34:31 - INFO - __main__ -   {'loss': 1.4655, 'learning_rate': 8.296317850549692e-05, 'epoch': 0.05}
12/08/2023 15:35:06 - INFO - __main__ -   {'loss': 1.1986, 'learning_rate': 9.419204379452388e-05, 'epoch': 0.07}
12/08/2023 15:35:40 - INFO - __main__ -   {'loss': 1.1202, 'learning_rate': 0.0001, 'epoch': 0.1}
12/08/2023 15:36:12 - INFO - __main__ -   {'loss': 1.0572, 'learning_rate': 0.0001, 'epoch': 0.12}
12/08/2023 15:36:47 - INFO - __main__ -   {'loss': 1.1036, 'learning_rate': 0.0001, 'epoch': 0.15}
12/08/2023 15:37:22 - INFO - __main__ -   {'loss': 1.1261, 'learning_rate': 0.0001, 'epoch': 0.17}
12/08/2023 15:37:57 - INFO - __main__ -   {'loss': 1.0687, 'learning_rate': 0.0001, 'epoch': 0.2}
12/08/2023 15:38:31 - INFO - __main__ -   {'loss': 1.0348, 'learning_rate': 0.0001, 'epoch': 0.22}
12/08/2023 15:39:03 - INFO - __main__ -   {'loss': 1.0341, 'learning_rate': 0.0001, 'epoch': 0.25}
12/08/2023 15:39:38 - INFO - __main__ -   {'loss': 1.0644, 'learning_rate': 0.0001, 'epoch': 0.27}
12/08/2023 15:40:13 - INFO - __main__ -   {'loss': 1.0593, 'learning_rate': 0.0001, 'epoch': 0.3}
12/08/2023 15:40:48 - INFO - __main__ -   {'loss': 1.0551, 'learning_rate': 0.0001, 'epoch': 0.32}
12/08/2023 15:41:22 - INFO - __main__ -   {'loss': 0.9852, 'learning_rate': 0.0001, 'epoch': 0.34}
12/08/2023 15:41:53 - INFO - __main__ -   {'loss': 1.0094, 'learning_rate': 0.0001, 'epoch': 0.37}
12/08/2023 15:42:28 - INFO - __main__ -   {'loss': 1.0628, 'learning_rate': 0.0001, 'epoch': 0.39}
12/08/2023 15:43:03 - INFO - __main__ -   {'loss': 1.0647, 'learning_rate': 0.0001, 'epoch': 0.42}
12/08/2023 15:43:39 - INFO - __main__ -   {'loss': 1.0288, 'learning_rate': 0.0001, 'epoch': 0.44}
12/08/2023 15:44:13 - INFO - __main__ -   {'loss': 1.0054, 'learning_rate': 0.0001, 'epoch': 0.47}
12/08/2023 15:44:44 - INFO - __main__ -   {'loss': 0.9882, 'learning_rate': 0.0001, 'epoch': 0.49}
12/08/2023 15:45:19 - INFO - __main__ -   {'loss': 1.0663, 'learning_rate': 0.0001, 'epoch': 0.52}
12/08/2023 15:45:54 - INFO - __main__ -   {'loss': 1.0892, 'learning_rate': 0.0001, 'epoch': 0.54}
12/08/2023 15:46:30 - INFO - __main__ -   {'loss': 1.0091, 'learning_rate': 0.0001, 'epoch': 0.57}
12/08/2023 15:47:03 - INFO - __main__ -   {'loss': 0.9953, 'learning_rate': 0.0001, 'epoch': 0.59}
12/08/2023 15:47:35 - INFO - __main__ -   {'loss': 0.969, 'learning_rate': 0.0001, 'epoch': 0.62}
12/08/2023 15:48:09 - INFO - __main__ -   {'loss': 1.0344, 'learning_rate': 0.0001, 'epoch': 0.64}
12/08/2023 15:48:45 - INFO - __main__ -   {'loss': 1.0522, 'learning_rate': 0.0001, 'epoch': 0.66}
12/08/2023 15:49:20 - INFO - __main__ -   {'loss': 0.9929, 'learning_rate': 0.0001, 'epoch': 0.69}
12/08/2023 15:49:54 - INFO - __main__ -   {'loss': 0.9793, 'learning_rate': 0.0001, 'epoch': 0.71}
12/08/2023 15:50:25 - INFO - __main__ -   {'loss': 0.9735, 'learning_rate': 0.0001, 'epoch': 0.74}
12/08/2023 15:51:00 - INFO - __main__ -   {'loss': 1.0424, 'learning_rate': 0.0001, 'epoch': 0.76}
12/08/2023 15:51:36 - INFO - __main__ -   {'loss': 1.0558, 'learning_rate': 0.0001, 'epoch': 0.79}
12/08/2023 15:52:11 - INFO - __main__ -   {'loss': 0.9988, 'learning_rate': 0.0001, 'epoch': 0.81}
12/08/2023 15:52:44 - INFO - __main__ -   {'loss': 0.968, 'learning_rate': 0.0001, 'epoch': 0.84}
12/08/2023 15:53:15 - INFO - __main__ -   {'loss': 0.9533, 'learning_rate': 0.0001, 'epoch': 0.86}
12/08/2023 15:53:49 - INFO - __main__ -   {'loss': 1.0208, 'learning_rate': 0.0001, 'epoch': 0.89}
12/08/2023 15:54:25 - INFO - __main__ -   {'loss': 1.0421, 'learning_rate': 0.0001, 'epoch': 0.91}
12/08/2023 15:54:59 - INFO - __main__ -   {'loss': 1.0019, 'learning_rate': 0.0001, 'epoch': 0.94}
12/08/2023 15:55:33 - INFO - __main__ -   {'loss': 0.9627, 'learning_rate': 0.0001, 'epoch': 0.96}
12/08/2023 15:56:05 - INFO - __main__ -   {'loss': 0.9675, 'learning_rate': 0.0001, 'epoch': 0.98}
12/08/2023 15:56:43 - INFO - __main__ -   {'loss': 1.0793, 'learning_rate': 0.0001, 'epoch': 1.01}
12/08/2023 15:57:17 - INFO - __main__ -   {'loss': 0.9146, 'learning_rate': 0.0001, 'epoch': 1.03}
12/08/2023 15:57:49 - INFO - __main__ -   {'loss': 0.9781, 'learning_rate': 0.0001, 'epoch': 1.06}
12/08/2023 15:58:25 - INFO - __main__ -   {'loss': 1.0299, 'learning_rate': 0.0001, 'epoch': 1.08}
12/08/2023 15:59:00 - INFO - __main__ -   {'loss': 0.9985, 'learning_rate': 0.0001, 'epoch': 1.11}
12/08/2023 15:59:34 - INFO - __main__ -   {'loss': 0.9781, 'learning_rate': 0.0001, 'epoch': 1.13}
12/08/2023 16:00:08 - INFO - __main__ -   {'loss': 0.9497, 'learning_rate': 0.0001, 'epoch': 1.16}
12/08/2023 16:00:41 - INFO - __main__ -   {'loss': 0.9844, 'learning_rate': 0.0001, 'epoch': 1.18}
12/08/2023 16:01:16 - INFO - __main__ -   {'loss': 1.0578, 'learning_rate': 0.0001, 'epoch': 1.21}
12/08/2023 16:01:51 - INFO - __main__ -   {'loss': 1.0062, 'learning_rate': 0.0001, 'epoch': 1.23}
12/08/2023 16:02:26 - INFO - __main__ -   {'loss': 0.9476, 'learning_rate': 0.0001, 'epoch': 1.26}
12/08/2023 16:02:59 - INFO - __main__ -   {'loss': 0.9224, 'learning_rate': 0.0001, 'epoch': 1.28}
12/08/2023 16:03:32 - INFO - __main__ -   {'loss': 0.9577, 'learning_rate': 0.0001, 'epoch': 1.3}
12/08/2023 16:04:07 - INFO - __main__ -   {'loss': 1.0324, 'learning_rate': 0.0001, 'epoch': 1.33}
12/08/2023 16:04:42 - INFO - __main__ -   {'loss': 0.984, 'learning_rate': 0.0001, 'epoch': 1.35}
12/08/2023 16:05:17 - INFO - __main__ -   {'loss': 0.9333, 'learning_rate': 0.0001, 'epoch': 1.38}
12/08/2023 16:05:49 - INFO - __main__ -   {'loss': 0.9512, 'learning_rate': 0.0001, 'epoch': 1.4}
12/08/2023 16:06:22 - INFO - __main__ -   {'loss': 0.9736, 'learning_rate': 0.0001, 'epoch': 1.43}
12/08/2023 16:06:57 - INFO - __main__ -   {'loss': 1.0293, 'learning_rate': 0.0001, 'epoch': 1.45}
12/08/2023 16:07:31 - INFO - __main__ -   {'loss': 1.0153, 'learning_rate': 0.0001, 'epoch': 1.48}
12/08/2023 16:08:06 - INFO - __main__ -   {'loss': 0.9497, 'learning_rate': 0.0001, 'epoch': 1.5}
12/08/2023 16:08:39 - INFO - __main__ -   {'loss': 0.9523, 'learning_rate': 0.0001, 'epoch': 1.53}
12/08/2023 16:09:11 - INFO - __main__ -   {'loss': 0.9663, 'learning_rate': 0.0001, 'epoch': 1.55}
12/08/2023 16:09:46 - INFO - __main__ -   {'loss': 1.0403, 'learning_rate': 0.0001, 'epoch': 1.58}
12/08/2023 16:10:22 - INFO - __main__ -   {'loss': 0.9824, 'learning_rate': 0.0001, 'epoch': 1.6}
12/08/2023 16:10:56 - INFO - __main__ -   {'loss': 0.9757, 'learning_rate': 0.0001, 'epoch': 1.62}
12/08/2023 16:11:29 - INFO - __main__ -   {'loss': 0.9254, 'learning_rate': 0.0001, 'epoch': 1.65}
12/08/2023 16:12:02 - INFO - __main__ -   {'loss': 0.9747, 'learning_rate': 0.0001, 'epoch': 1.67}
12/08/2023 16:12:37 - INFO - __main__ -   {'loss': 1.0223, 'learning_rate': 0.0001, 'epoch': 1.7}
12/08/2023 16:13:12 - INFO - __main__ -   {'loss': 0.9814, 'learning_rate': 0.0001, 'epoch': 1.72}
12/08/2023 16:13:46 - INFO - __main__ -   {'loss': 0.9707, 'learning_rate': 0.0001, 'epoch': 1.75}
12/08/2023 16:14:20 - INFO - __main__ -   {'loss': 0.9347, 'learning_rate': 0.0001, 'epoch': 1.77}
12/08/2023 16:14:53 - INFO - __main__ -   {'loss': 0.987, 'learning_rate': 0.0001, 'epoch': 1.8}
12/08/2023 16:15:28 - INFO - __main__ -   {'loss': 1.0303, 'learning_rate': 0.0001, 'epoch': 1.82}
12/08/2023 16:16:03 - INFO - __main__ -   {'loss': 0.9991, 'learning_rate': 0.0001, 'epoch': 1.85}
12/08/2023 16:16:38 - INFO - __main__ -   {'loss': 0.939, 'learning_rate': 0.0001, 'epoch': 1.87}
12/08/2023 16:17:11 - INFO - __main__ -   {'loss': 0.9164, 'learning_rate': 0.0001, 'epoch': 1.9}
12/08/2023 16:17:44 - INFO - __main__ -   {'loss': 0.9904, 'learning_rate': 0.0001, 'epoch': 1.92}
12/08/2023 16:18:19 - INFO - __main__ -   {'loss': 1.0421, 'learning_rate': 0.0001, 'epoch': 1.94}
12/08/2023 16:18:54 - INFO - __main__ -   {'loss': 1.004, 'learning_rate': 0.0001, 'epoch': 1.97}
12/08/2023 16:19:27 - INFO - __main__ -   {'loss': 0.9329, 'learning_rate': 0.0001, 'epoch': 1.99}
12/08/2023 16:20:02 - INFO - __main__ -   {'loss': 1.0133, 'learning_rate': 0.0001, 'epoch': 2.02}
12/08/2023 16:20:37 - INFO - __main__ -   {'loss': 0.9344, 'learning_rate': 0.0001, 'epoch': 2.04}
12/08/2023 16:21:11 - INFO - __main__ -   {'loss': 0.889, 'learning_rate': 0.0001, 'epoch': 2.07}
12/08/2023 16:21:42 - INFO - __main__ -   {'loss': 0.9375, 'learning_rate': 0.0001, 'epoch': 2.09}
12/08/2023 16:22:17 - INFO - __main__ -   {'loss': 0.9959, 'learning_rate': 0.0001, 'epoch': 2.12}
12/08/2023 16:22:51 - INFO - __main__ -   {'loss': 1.0186, 'learning_rate': 0.0001, 'epoch': 2.14}
12/08/2023 16:23:26 - INFO - __main__ -   {'loss': 0.9828, 'learning_rate': 0.0001, 'epoch': 2.17}
12/08/2023 16:24:00 - INFO - __main__ -   {'loss': 0.9071, 'learning_rate': 0.0001, 'epoch': 2.19}
12/08/2023 16:24:31 - INFO - __main__ -   {'loss': 0.8888, 'learning_rate': 0.0001, 'epoch': 2.22}
12/08/2023 16:25:07 - INFO - __main__ -   {'loss': 0.9878, 'learning_rate': 0.0001, 'epoch': 2.24}
12/08/2023 16:25:42 - INFO - __main__ -   {'loss': 0.9706, 'learning_rate': 0.0001, 'epoch': 2.26}
12/08/2023 16:26:17 - INFO - __main__ -   {'loss': 0.9408, 'learning_rate': 0.0001, 'epoch': 2.29}
12/08/2023 16:26:52 - INFO - __main__ -   {'loss': 0.8945, 'learning_rate': 0.0001, 'epoch': 2.31}
12/08/2023 16:27:23 - INFO - __main__ -   {'loss': 0.9189, 'learning_rate': 0.0001, 'epoch': 2.34}
12/08/2023 16:27:59 - INFO - __main__ -   {'loss': 1.0128, 'learning_rate': 0.0001, 'epoch': 2.36}
12/08/2023 16:28:34 - INFO - __main__ -   {'loss': 1.0122, 'learning_rate': 0.0001, 'epoch': 2.39}
12/08/2023 16:29:09 - INFO - __main__ -   {'loss': 0.9336, 'learning_rate': 0.0001, 'epoch': 2.41}
12/08/2023 16:29:42 - INFO - __main__ -   {'loss': 0.921, 'learning_rate': 0.0001, 'epoch': 2.44}
12/08/2023 16:30:13 - INFO - __main__ -   {'loss': 0.9011, 'learning_rate': 0.0001, 'epoch': 2.46}
12/08/2023 16:30:49 - INFO - __main__ -   {'loss': 1.0102, 'learning_rate': 0.0001, 'epoch': 2.49}
12/08/2023 16:31:24 - INFO - __main__ -   {'loss': 0.9883, 'learning_rate': 0.0001, 'epoch': 2.51}
12/08/2023 16:31:58 - INFO - __main__ -   {'loss': 0.9309, 'learning_rate': 0.0001, 'epoch': 2.53}
12/08/2023 16:32:32 - INFO - __main__ -   {'loss': 0.8971, 'learning_rate': 0.0001, 'epoch': 2.56}
12/08/2023 16:33:04 - INFO - __main__ -   {'loss': 0.9044, 'learning_rate': 0.0001, 'epoch': 2.58}
12/08/2023 16:33:39 - INFO - __main__ -   {'loss': 1.0017, 'learning_rate': 0.0001, 'epoch': 2.61}
12/08/2023 16:34:14 - INFO - __main__ -   {'loss': 1.0271, 'learning_rate': 0.0001, 'epoch': 2.63}
12/08/2023 16:34:49 - INFO - __main__ -   {'loss': 0.9486, 'learning_rate': 0.0001, 'epoch': 2.66}
12/08/2023 16:35:23 - INFO - __main__ -   {'loss': 0.9145, 'learning_rate': 0.0001, 'epoch': 2.68}
12/08/2023 16:35:54 - INFO - __main__ -   {'loss': 0.9101, 'learning_rate': 0.0001, 'epoch': 2.71}
12/08/2023 16:36:29 - INFO - __main__ -   {'loss': 1.0207, 'learning_rate': 0.0001, 'epoch': 2.73}
12/08/2023 16:37:04 - INFO - __main__ -   {'loss': 1.0157, 'learning_rate': 0.0001, 'epoch': 2.76}
12/08/2023 16:37:39 - INFO - __main__ -   {'loss': 0.9455, 'learning_rate': 0.0001, 'epoch': 2.78}
12/08/2023 16:38:13 - INFO - __main__ -   {'loss': 0.8971, 'learning_rate': 0.0001, 'epoch': 2.81}
12/08/2023 16:38:44 - INFO - __main__ -   {'loss': 0.9227, 'learning_rate': 0.0001, 'epoch': 2.83}
12/08/2023 16:39:19 - INFO - __main__ -   {'loss': 0.9938, 'learning_rate': 0.0001, 'epoch': 2.85}
12/08/2023 16:39:54 - INFO - __main__ -   {'loss': 0.9986, 'learning_rate': 0.0001, 'epoch': 2.88}
12/08/2023 16:40:29 - INFO - __main__ -   {'loss': 0.947, 'learning_rate': 0.0001, 'epoch': 2.9}
12/08/2023 16:41:03 - INFO - __main__ -   {'loss': 0.9297, 'learning_rate': 0.0001, 'epoch': 2.93}
12/08/2023 16:41:34 - INFO - __main__ -   {'loss': 0.9431, 'learning_rate': 0.0001, 'epoch': 2.95}
12/08/2023 16:42:10 - INFO - __main__ -   {'loss': 1.0226, 'learning_rate': 0.0001, 'epoch': 2.98}
12/08/2023 16:42:37 - INFO - __main__ -   {'train_runtime': 4156.3616, 'train_samples_per_second': 37.534, 'train_steps_per_second': 0.293, 'total_flos': 7.890014711641539e+17, 'train_loss': 0.9967418730944052, 'epoch': 3.0}
